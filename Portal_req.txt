You can host the entire AI portal stack on your own Azure environment by combining Azure OpenAI (and other Azure AI services) with an Azure‑native GenAI gateway pattern. Your LiteLLM or similar proxy can run on Azure as part of this architecture and sit behind Azure API Management (APIM) for governance and security.
​

Target Azure Architecture
Use Microsoft’s GenAI Gateway pattern as your reference and adapt it to your portal:
​

Front door

Azure Front Door or Application Gateway (optional, for internet-facing access)

Azure API Management (APIM) as your AI gateway entry point

AI gateway / model routing

Option A: Azure API Management AI Gateway features only (no LiteLLM)

Option B: APIM in front of a LiteLLM Proxy container running on:

Azure App Service (Linux container)

Azure Container Apps

Azure Kubernetes Service (AKS) for more control

Model backends

Azure OpenAI deployments (GPT‑4/5, o series, etc.) in Azure AI Foundry

Optional: External LLMs (Anthropic, OpenAI API, Google, Bedrock) via LiteLLM

Optional: Self‑hosted models (e.g., on AKS) reachable through APIM or LiteLLM

State and data

Azure PostgreSQL or Cosmos DB for audit logs, usage, conversation history

Azure Cache for Redis for caching and rate limiting

Azure Blob Storage for documents (RAG corpora, prompt templates)

Security and secrets

Azure AD for SSO and RBAC

Azure Key Vault for API keys and connection strings, integrated with LiteLLM/APIM
​

Microsoft’s GenAI gateway docs explicitly support Azure OpenAI plus on‑prem/custom LLMs through APIM and self‑hosted gateways, which matches your “Azure hosted servers” requirement.
​

Step-by-step Azure Deployment Plan
1. Foundation: Network, Identity, and Compliance
Deploy into a hub‑and‑spoke VNet or your existing secure VNet:

APIM, App Service/Container Apps, databases, and Azure OpenAI all in private subnets with Private Endpoints where possible.
​

Configure Azure AD:

Enterprise apps + groups for Clinician, Billing, Admin, Developer roles.

Enforce MFA and conditional access (e.g., only from corporate or VPN ranges).

Ensure HIPAA posture:

Confirm your Azure subscription is under a signed Microsoft BAA.
​

Treat Azure OpenAI as HIPAA‑eligible only after BAA is in place and you configure required safeguards.
​

2. Deploy LiteLLM Proxy on Azure
Microsoft already documents deploying LiteLLM Proxy in Azure to front Azure OpenAI; you can follow that pattern:
​

Create litellm_config.yaml and define Azure OpenAI deployments and any external models:
​

text
model_list:
  - model_name: azure-gpt-4o
    litellm_params:
      model: azure/gpt-4o
      api_base: ${AZURE_OPENAI_ENDPOINT}
      api_key: ${AZURE_OPENAI_KEY}
      api_version: 2024-05-01-preview

  - model_name: claude-3-sonnet
    litellm_params:
      model: anthropic/claude-3-sonnet
      api_key: ${ANTHROPIC_API_KEY}

  - model_name: gemini-1.5-pro
    litellm_params:
      model: vertex_ai/gemini-1.5-pro
      api_key: ${GOOGLE_API_KEY}
Containerize LiteLLM:

Use the official LiteLLM image as in Microsoft’s “integrate Ollama / LiteLLM with Azure OpenAI” guide, which runs via Docker with a mounted config file:
​

bash
docker run -d \
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \
  -p 4000:4000 \
  --name litellm-proxy \
  --restart always \
  ghcr.io/berriai/litellm:main-latest
Host the container on:

Azure Container Apps (simplest for autoscaling and private networking), or

App Service for Containers, or

AKS if you want more control for future scaling.

Store API keys and Azure OpenAI endpoint values in Azure Key Vault and inject them via environment variables.
​

3. Put Azure API Management in Front
APIM becomes your enterprise GenAI gateway and public/enterprise front door:
​

Import LiteLLM Proxy as an HTTP backend in APIM, and create OpenAI‑compatible API definitions.

Apply APIM AI Gateway capabilities:
​

Traffic mediation and throttling per user or per app.

Centralized logging and request/response transformations.

Policy-based routing (e.g., route certain operations directly to Azure OpenAI, others to self‑hosted models).

Use policies to:

Inject user identity (from Azure AD) into headers or metadata.

Mask or remove PHI fields in prompts where appropriate.

Enforce size limits on requests and responses.

Microsoft’s GenAI gateway reference architecture explicitly shows APIM used to federate Azure OpenAI and custom LLM deployments, including on-prem/self‑hosted setups.
​

4. Build the Portal App on Azure
Host the React/Next.js front-end on:

Azure Static Web Apps, or

Azure App Service (Web App).

Front-end calls APIM endpoints using the OpenAI-compatible API (no direct calls to Azure OpenAI).

Use Azure AD authentication for the portal:

Integrate with MSAL in the frontend to get access tokens.

Configure APIM to validate tokens and propagate user identity to LiteLLM/backend.

Store:

User profiles, preferences, permissions in Azure SQL or Cosmos DB.

Conversation history and usage logs in Azure PostgreSQL/Cosmos DB (with proper retention and encryption).

This keeps all traffic inside your Azure network and allows you to reuse your existing Azure governance, monitoring (App Insights, Log Analytics), and security tooling.
​

Azure-Specific Compliance and Security
BAAs:

Ensure your Azure tenant has a BAA with Microsoft, and separately sign a BAA with any non‑Azure AI providers (e.g., OpenAI API, Anthropic) if they will receive PHI.
​

Private networking:

Use Private Endpoints for Azure OpenAI and databases, and restrict APIM / Container Apps to private VNets where possible.
​

Logging and monitoring:

Centralize logs in Azure Monitor / Log Analytics for:

All APIM calls (including prompts, metadata, but with PHI minimization where necessary).

LiteLLM Proxy (usage, errors, model routing decisions).

Key management:

Azure Key Vault for all secrets, integrated with LiteLLM and APIM policies.
​

Minimal Azure Deployment Path (Concrete)
Provision Azure resources

Resource group, VNet, subnets.

Azure OpenAI resource + at least one GPT‑4o or gpt‑4.1 deployment.

Azure Container Apps environment.

Azure API Management (Developer or Standard tier to start).

Deploy LiteLLM to Azure Container Apps

Push your config and image or use the public LiteLLM image with a mounted config.

Configure environment variables from Key Vault.

Configure APIM GenAI Gateway

Create an API in APIM pointing to https://<litellm>.azurecontainerapps.io/v1/chat/completions.

Add inbound policies for JWT validation (Azure AD), PHI scrub, rate limits.

Use APIM’s GenAI gateway features for AI backend management and model catalog if desired.
​

Deploy your portal UI

Host SPA on Static Web Apps or App Service.

Use MSAL to get Azure AD tokens and call APIM with Authorization: Bearer <token>.

Iterate on healthcare use cases

Add RAG endpoints (e.g., separate Container App with a FastAPI backend) and expose via APIM.

Layer in clinical, billing, or coding agents, still behind APIM → LiteLLM → Azure/OpenAI.

This keeps everything “on your own Azure hosted servers” while still letting you route to external models when needed under strict governance and logging.

Related

What are the network requirements for a self-hosted GenAI gateway

How to configure Azure APIM self-hosted gateway step by step

Options for deploying custom LLMs on Azure AI Foundry

How to secure data between on-premises LLMs and Azure OpenAI

Cost estimate for running a GenAI gateway on Azure and on-premises servers